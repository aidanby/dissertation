\chapter{Conclusion}
\label{sec:conclusion}

This dissertation has explored the integration of Large Language Models (LLMs) with traditional program analysis techniques to address critical challenges in software evolution. Through a series of novel approaches and empirical evaluations, I have demonstrated that combining the strengths of LLMs with established software engineering methodologies yields superior results compared to either approach in isolation. My work spans multiple dimensions of software evolution, including fault localization, automated program repair, program transpilation, and security vulnerability detection.

\section{Summary of Contributions}

\subsection{Bidirectional Fine-tuning for Fault Localization}

I developed a bidirectional fine-tuning technique that enables LLMs to perform effective fault localization without relying on pre-written tests (Chapter~\ref{ch:llmao})~\cite{Llmao}. By adapting traditionally left-to-right LLMs to understand code in a bidirectional manner, I demonstrated significant improvements in identifying faulty lines of code. This approach not only assists in debugging but also proves effective in detecting runtime security vulnerabilities, addressing a critical gap in existing fault localization techniques.

\subsection{LLM Entropy for Automated Program Repair}

I introduced a novel application of LLM entropy values to enhance automated program repair (Chapter~\ref{ch:entropy})~\cite{yang2024revisiting}. My research showed that LLM entropy (i.e., a measure of model uncertainty) can be leveraged to improve all three critical stages of program repair: fault localization, patch testing efficiency, and plausible patch ranking. The empirical results demonstrated that this hybrid approach outperforms both traditional APR techniques and pure LLM-based methods, achieving an 18\% higher precision in classifying plausible patches while remaining effective for patches produced by modern ML-based tools.

\subsection{Verified Rust Transpilation}

VERT, my verified equivalent Rust transpilation framework, combines LLMs with verification harnesses to ensure functional equivalence between source and target code (Chapter~\ref{ch:vert})~\cite{vert}. This approach addresses the hallucination problem inherent in LLMs by incorporating formal verification techniques, resulting in transpiled code that is not only functionally correct but also more idiomatic than code produced by traditional transpilers. The evaluation on real-world repositories confirms that this approach strikes an optimal balance between correctness guarantees and code naturalness.

\subsection{Multi-task Security Vulnerability Detection}

Building upon the fault localization work, I extended the approach to security vulnerability detection through multi-task instruction-tuning of LLMs (Chapter~\ref{ch:msivd})~\cite{yang2024security}. By training models to simultaneously identify vulnerable code and explain exploitation vectors, I created a more comprehensive vulnerability detection system. The experiments demonstrate that this approach effectively detects vulnerabilities spanning multiple files across large repositories, addressing the limitations of line-level fault localization.

\subsection{Hybrid Node.js Vulnerability Detection}

NodeMedic represents a hybrid approach that combines program analysis with LLMs for detecting exploitable vulnerabilities in Node.js packages (Chapter~\ref{ch:nodejs}). By integrating taint provenance tracking with graph neural networks and LLMs, I achieved superior detection performance compared to existing methods. The comprehensive evaluation shows that NodeMedic-GNN achieves an F1 score of 0.943, significantly outperforming traditional program analysis approaches and demonstrating the value of combining multiple techniques for vulnerability detection.

\section{Key Insights and Implications}

Several important insights emerge from this research that have broader implications for the field of software engineering.

While LLMs excel at generating natural, human-like code, they fundamentally operate as black-box models with inherent limitations in understanding program semantics and correctness. My work demonstrates that these limitations can be mitigated by integrating LLMs with traditional program analysis techniques, creating systems that leverage the strengths of both approaches.

The entropy values produced by LLMs during generation provide valuable signals about model uncertainty, which can be harnessed for tasks beyond code generation. This research shows that these entropy values effectively identify potential fault locations and help prioritize candidate patches, offering a novel perspective on how LLM internals can inform software engineering tools.

Verification of LLM outputs is essential for critical software engineering tasks. The transpilation work demonstrates that combining LLM generation capabilities with formal verification techniques produces results that are both correct and natural, addressing a fundamental tension in automated code transformation.

Multi-task learning enables LLMs to develop more nuanced understanding of software vulnerabilities by simultaneously learning to identify vulnerable code and explain exploitation vectors. This approach produces models that can reason about security implications across file boundaries, addressing limitations of traditional vulnerability detection methods.

Most importantly, this research confirms that hybrid approaches combining neural and symbolic methods consistently outperform pure neural or pure symbolic approaches across various software evolution tasks. This finding has significant implications for the design of future software engineering tools, suggesting that the most effective systems will integrate multiple paradigms rather than relying exclusively on either traditional program analysis or deep learning.

\section{Limitations and Future Work}

Despite the advances presented in this dissertation, several limitations remain to be addressed in future work, while recent developments in LLMs present both new opportunities and continued challenges.

The scalability of these approaches to very large codebases remains a challenge, particularly for tasks requiring whole-program analysis. However, recent advances in LLM context windows (with models now supporting millions of tokens) present new opportunities for applying these techniques to larger codebases without the segmentation strategies employed in this work. Future research should explore how these expanded context windows can be leveraged while maintaining the benefits of the hybrid approaches developed here.

While the transpilation work focused on Rust as a target language, extending these techniques to other language pairs would provide valuable insights into the generalizability of the approach. Additionally, exploring bidirectional transpilation (i.e., converting code back to its original language) could serve as an additional verification mechanism. The emergence of agentic LLMs that can autonomously call verification tools presents exciting possibilities for self-checking transpilation systems that could iteratively improve their outputs.

The security vulnerability detection work currently focuses on specific vulnerability types in Node.js applications. Expanding this approach to cover a broader range of vulnerability classes and programming languages would increase its practical utility. Modern LLMs demonstrate improved zero-shot performance on fault localization tasks, potentially reducing the need for extensive fine-tuning. However, the specialized knowledge captured through the fine-tuning approaches in this dissertation (particularly the multi-task learning for vulnerability explanation) remains valuable for achieving high precision in security-critical applications.

The interpretability of hybrid neural-symbolic systems remains limited, making it difficult for developers to understand why certain code is flagged as vulnerable or why specific repairs are suggested. Future work should explore techniques for making these systems more transparent and explainable, potentially leveraging the improved reasoning capabilities of newer LLMs to provide better explanations of their decisions.

Despite rapid advances in LLM capabilities, there remains significant value in the program analysis integration and fine-tuning optimization techniques presented in this work. Models continue to be expensive to pre-train, and the techniques developed in this thesis can be valuable for generating higher quality training data without dependency on human labelers. The entropy-based approaches for patch ranking and the multi-task learning frameworks can serve as automated data curation mechanisms, identifying high-quality code examples and vulnerability patterns for training future models.

Reinforcement learning presents an underexplored opportunity, particularly for security-related data flows. The provenance graphs and taint analysis techniques developed in this work could serve as reward signals for reinforcement learning systems, enabling models to learn optimal security practices through interaction with program analysis tools. This could lead to LLMs that not only detect vulnerabilities but also learn to write more secure code through reinforcement.

The parameter-efficient fine-tuning techniques (e.g., LoRA) demonstrated in this work remain relevant even as base models grow larger. These approaches enable rapid adaptation of large models to specific domains and tasks without the computational overhead of full fine-tuning, making specialized software engineering applications more accessible to researchers and practitioners with limited computational resources.

Future work should also investigate how the hybrid approaches developed here can be adapted to leverage emerging LLM capabilities while maintaining their core advantages. For instance, combining the bidirectional fine-tuning techniques with modern instruction-following models could yield even more effective fault localization systems, while the verification frameworks developed for transpilation could be extended to work with agentic systems that can autonomously invoke multiple verification tools.

\section{Concluding Remarks}

This dissertation has demonstrated that software engineering is indeed an evolving process that benefits from a holistic understanding of software properties. By addressing the limitations of LLMs through integration with program analysis techniques, I have created effective tools for fault localization, automated program repair, program transpilation, and security vulnerability detection.

The work contributes to a growing body of evidence suggesting that the future of software engineering tools lies not in replacing traditional techniques with deep learning, but in thoughtfully combining them to create systems that leverage the strengths of both approaches. As software systems continue to grow in complexity and importance, such hybrid approaches will be essential for maintaining software quality, security, and evolvability.

The tools and techniques presented in this dissertation are open-source and available to the research community, providing a foundation for future work in this rapidly evolving field. I hope that these contributions will inspire further research into the integration of LLMs with traditional software engineering methodologies, ultimately leading to more powerful and practical tools for software evolution.
